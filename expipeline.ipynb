{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69891e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Step 1: Data Acquisition & Splitting\n",
      "Training Data Shape: (455, 30)\n",
      "Test Data Shape:     (114, 30)\n",
      "------------------------------\n",
      ">>> Step 2: Building the Pipeline\n",
      ">>> Step 3: Training & Hyperparameter Tuning\n",
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "\n",
      "Best Parameters found: {'classifier__max_depth': None, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 200}\n",
      "Best CV Score: 0.9626\n",
      "------------------------------\n",
      ">>> Step 4: Final Evaluation on Test Set\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94        42\n",
      "           1       0.96      0.97      0.97        72\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.96      0.95      0.95       114\n",
      "weighted avg       0.96      0.96      0.96       114\n",
      "\n",
      "------------------------------\n",
      ">>> Step 5: Exporting Model (Pickling)\n",
      "SUCCESS: Model saved to 'breast_cancer_pipeline_v1.joblib'\n",
      "พร้อมส่งไฟล์นี้ให้ทีม Dev/DE แล้วครับ!\n",
      "------------------------------\n",
      ">>> Step 6: Simulating Production Usage (Load & Predict)\n",
      "Input Data shape: (1, 30)\n",
      "Prediction Class: 0\n",
      "Probability: [1. 0.]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib  # พระเอกของเราในการ Export Model\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# ==========================================\n",
    "# 1. Data Preparation (Simulation)\n",
    "# ==========================================\n",
    "print(\">>> Step 1: Data Acquisition & Splitting\")\n",
    "\n",
    "# โหลดข้อมูล\n",
    "data = load_breast_cancer()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# จำลองสถานการณ์: แกล้งทำข้อมูลให้แหว่งๆ (Missing Values) เพื่อโชว์ Data Prep\n",
    "rng = np.random.RandomState(42)\n",
    "n_samples = X.shape[0]\n",
    "n_features = X.shape[1]\n",
    "# สุ่มทำให้ข้อมูลหายไปบางจุด\n",
    "X[rng.randint(0, n_samples, size=50), rng.randint(0, n_features, size=50)] = np.nan\n",
    "\n",
    "# แบ่ง Train/Test (Test set คือของศักดิ์สิทธิ์ ห้ามแตะจนจบงาน)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training Data Shape: {X_train.shape}\")\n",
    "print(f\"Test Data Shape:     {X_test.shape}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# ==========================================\n",
    "# 2. Constructing the Full Pipeline\n",
    "# ==========================================\n",
    "print(\">>> Step 2: Building the Pipeline\")\n",
    "\n",
    "# นี่คือหัวใจสำคัญ: เรามัดรวมทุกขั้นตอนเป็นก้อนเดียว\n",
    "# 1. Imputer: เติมค่าที่หายไป (Data Prep)\n",
    "# 2. Scaler: ปรับสเกลข้อมูล (Preprocessing)\n",
    "# 3. Model: ตัวทำนาย (Modeling)\n",
    "\n",
    "full_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),  # ถ้าเจอ NaN ให้เติมด้วย Median\n",
    "    ('scaler', StandardScaler()),                   # ปรับ Standard Scale\n",
    "    ('classifier', RandomForestClassifier(random_state=42)) # โมเดล\n",
    "])\n",
    "\n",
    "# ==========================================\n",
    "# 3. Training & Tuning (Model Selection)\n",
    "# ==========================================\n",
    "print(\">>> Step 3: Training & Hyperparameter Tuning\")\n",
    "\n",
    "# กำหนดสิ่งที่จะจูน\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [50, 100, 200],\n",
    "    'classifier__max_depth': [None, 10, 20],\n",
    "    'classifier__min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "# ใช้ GridSearchCV เพื่อหาค่าที่ดีที่สุด (โดยใช้ Cross-Validation ในตัว)\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=full_pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# เริ่มสอนโมเดล (ขั้นตอนนี้กินเวลาสุด)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "print(f\"\\nBest Parameters found: {grid_search.best_params_}\")\n",
    "print(f\"Best CV Score: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. Final Evaluation\n",
    "# ==========================================\n",
    "print(\"-\" * 30)\n",
    "print(\">>> Step 4: Final Evaluation on Test Set\")\n",
    "\n",
    "# Predict ข้อมูล Test (Pipeline จะจัดการ Impute -> Scale -> Predict ให้เองอัตโนมัติ)\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# ==========================================\n",
    "# 5. Export Model (The Deployment Step)\n",
    "# ==========================================\n",
    "print(\"-\" * 30)\n",
    "print(\">>> Step 5: Exporting Model (Pickling)\")\n",
    "\n",
    "model_filename = 'breast_cancer_pipeline_v1.joblib'\n",
    "\n",
    "# Save ไฟล์ .joblib (ข้างในมีทั้ง Logic การเติมค่า, การ Scale, และตัวโมเดล)\n",
    "joblib.dump(best_model, model_filename)\n",
    "\n",
    "print(f\"SUCCESS: Model saved to '{model_filename}'\")\n",
    "print(\"พร้อมส่งไฟล์นี้ให้ทีม Dev/DE แล้วครับ!\")\n",
    "\n",
    "# ==========================================\n",
    "# 6. Simulation: Usage in Production\n",
    "# ==========================================\n",
    "print(\"-\" * 30)\n",
    "print(\">>> Step 6: Simulating Production Usage (Load & Predict)\")\n",
    "\n",
    "# สมมติว่านี่คือเครื่อง Server ของบริษัท หรือ API\n",
    "# เราโหลดไฟล์ที่ Save ไว้ขึ้นมา\n",
    "loaded_model = joblib.load(model_filename)\n",
    "\n",
    "# สมมติมีข้อมูลลูกค้าใหม่เข้ามา 1 คน (New Data)\n",
    "new_data = X_test[0].reshape(1, -1) # ข้อมูลจริงตัวแรก\n",
    "\n",
    "# สั่ง Predict ได้เลย ไม่ต้องมานั่งเขียนโค้ด Impute หรือ Scale ใหม่\n",
    "prediction = loaded_model.predict(new_data)\n",
    "prob = loaded_model.predict_proba(new_data)\n",
    "\n",
    "print(f\"Input Data shape: {new_data.shape}\")\n",
    "print(f\"Prediction Class: {prediction[0]}\")\n",
    "print(f\"Probability: {prob[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
